{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4512c811-c767-47a6-bcb0-717deb50fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcb01f8-e1df-4340-9e0f-35c896614f0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Label Mapping (Number → Crop Name):\n",
      "0 → bajra\n",
      "1 → coffee\n",
      "2 → cotton\n",
      "3 → groundnut\n",
      "4 → jowar\n",
      "5 → maize\n",
      "6 → onion\n",
      "7 → potato\n",
      "8 → pulses\n",
      "9 → rice\n",
      "10 → soybean\n",
      "11 → sugarcane\n",
      "12 → tea\n",
      "13 → tomato\n",
      "14 → wheat\n",
      "\n",
      "# Use this dictionary in other scripts:\n",
      "label_mapping = {\n",
      "    0: 'bajra',\n",
      "    1: 'coffee',\n",
      "    2: 'cotton',\n",
      "    3: 'groundnut',\n",
      "    4: 'jowar',\n",
      "    5: 'maize',\n",
      "    6: 'onion',\n",
      "    7: 'potato',\n",
      "    8: 'pulses',\n",
      "    9: 'rice',\n",
      "    10: 'soybean',\n",
      "    11: 'sugarcane',\n",
      "    12: 'tea',\n",
      "    13: 'tomato',\n",
      "    14: 'wheat',\n",
      "}\n",
      "\n",
      "Dataset Summary:\n",
      "            state       N      P       K  temperature  humidity    ph  \\\n",
      "0       Jharkhand   51.49  25.10   42.18        31.35      48.6  6.77   \n",
      "1         Tripura   67.81  36.38   47.18        23.57      56.8  6.48   \n",
      "2      Tamil Nadu   35.51  28.81   29.58        23.00      63.4  6.61   \n",
      "3           Bihar   69.83  33.48   39.43        26.26      67.9  6.32   \n",
      "4     Maharashtra  116.75  53.15  122.32        26.73      74.2  6.88   \n",
      "5  Andhra Pradesh   37.99  37.33   54.56        30.69      75.9  6.63   \n",
      "6       Rajasthan   52.71  31.65   49.20        29.10      57.5  6.15   \n",
      "7          Odisha   79.37  46.78   50.59        27.87      68.5  6.48   \n",
      "8     West Bengal   41.13  30.38   37.95        25.19      58.2  6.01   \n",
      "9           Assam   86.84  43.96   67.59        23.43      81.3  5.96   \n",
      "\n",
      "   rainfall               season       crop  \n",
      "0      2.59    June to September      bajra  \n",
      "1      4.68   September to March     tomato  \n",
      "2      1.60  October to February     pulses  \n",
      "3      0.00   September to March     tomato  \n",
      "4      5.60  February to October  sugarcane  \n",
      "5      2.25    June to September  groundnut  \n",
      "6      0.68    June to September     cotton  \n",
      "7      0.86      June to October      maize  \n",
      "8      1.34  October to February     pulses  \n",
      "9      3.81   April to September        tea  \n",
      "\n",
      "Total rows: 10000\n",
      "Unique crops: 15\n",
      "States covered: 24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1️⃣ Load Dataset\n",
    "df = pd.read_csv(\"abcd.csv\")  # <-- replace with your generated dataset\n",
    "\n",
    "# 2️⃣ Clean & Prepare Numeric Columns\n",
    "numeric_cols = [\"N\", \"P\", \"K\", \"temperature\", \"humidity\", \"ph\", \"rainfall\"]\n",
    "\n",
    "# Convert to numeric safely\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Drop invalid rows\n",
    "df = df.dropna(subset=numeric_cols)\n",
    "\n",
    "# 3️⃣ Prepare Features & Labels\n",
    "X = df[numeric_cols]\n",
    "y = df[\"crop\"]   # <-- 'crop' is your target column\n",
    "\n",
    "# 4️⃣ Encode Crop Labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# 5️⃣ Save Label Encoder for Later Use\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "# 6️⃣ Print Numeric Label → Crop Mapping\n",
    "print(\"✅ Label Mapping (Number → Crop Name):\")\n",
    "for i, crop in enumerate(le.classes_):\n",
    "    print(f\"{i} → {crop}\")\n",
    "\n",
    "# 7️⃣ Create Mapping Dictionary for Reference\n",
    "label_mapping = {i: crop for i, crop in enumerate(le.classes_)}\n",
    "\n",
    "print(\"\\n# Use this dictionary in other scripts:\")\n",
    "print(\"label_mapping = {\")\n",
    "for num, crop in label_mapping.items():\n",
    "    print(f\"    {num}: '{crop}',\")\n",
    "print(\"}\")\n",
    "\n",
    "# ✅ Optional: Show summary\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nTotal rows: {len(df)}\")\n",
    "print(f\"Unique crops: {df['crop'].nunique()}\")\n",
    "print(f\"States covered: {df['state'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b0bec76-d902-4077-802d-0db0f935e67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           N      P      K  temperature  humidity    ph  rainfall\n",
      "9254   74.89  25.94  44.63        31.59      77.2  6.16     11.84\n",
      "1561   75.32  38.44  49.81        27.29      80.0  6.93     11.51\n",
      "1670   35.44  30.54  43.09        26.83      78.4  6.66      4.70\n",
      "6087  100.69  44.61  55.28        16.75      66.5  7.80      2.40\n",
      "6669   76.44  43.42  75.74        22.72      81.9  4.61      8.47\n",
      "(8000, 7)\n",
      " \n",
      "           N      P      K  temperature  humidity    ph  rainfall\n",
      "6252   79.53  49.35  54.01        17.81      73.0  7.16      7.31\n",
      "4684  101.40  52.71  44.95        20.24      40.0  7.80      0.80\n",
      "1731   73.57  44.08  45.66        26.77      52.8  6.54      0.34\n",
      "4742   30.55  25.53  36.73        19.75      55.4  6.66      0.85\n",
      "4521   98.67  59.65  82.26        31.51      81.7  7.23      0.13\n",
      "(2000, 7)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "print(X_train.head(5))\n",
    "print(X_train.shape)\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "print(X_test.head(5))\n",
    "print(X_test.shape)\n",
    "print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5cdca06-8cb4-48b5-af42-2321e772bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94eac1db-4da8-4a2f-aa1a-7998c179d94e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Decision Tree...\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shind\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:15:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1771\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score -2.706801\n",
      "[LightGBM] [Info] Start training from score -2.746921\n",
      "[LightGBM] [Info] Start training from score -2.754749\n",
      "[LightGBM] [Info] Start training from score -2.714320\n",
      "[LightGBM] [Info] Start training from score -2.782639\n",
      "[LightGBM] [Info] Start training from score -2.593606\n",
      "[LightGBM] [Info] Start training from score -2.659260\n",
      "[LightGBM] [Info] Start training from score -2.752786\n",
      "[LightGBM] [Info] Start training from score -2.750827\n",
      "[LightGBM] [Info] Start training from score -2.756715\n",
      "[LightGBM] [Info] Start training from score -2.639808\n",
      "[LightGBM] [Info] Start training from score -2.703063\n",
      "[LightGBM] [Info] Start training from score -2.682748\n",
      "[LightGBM] [Info] Start training from score -2.703063\n",
      "[LightGBM] [Info] Start training from score -2.691931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    precision = np.mean([v[\"precision\"] for k, v in report.items() if isinstance(v, dict)])\n",
    "    recall = np.mean([v[\"recall\"] for k, v in report.items() if isinstance(v, dict)])\n",
    "    f1 = np.mean([v[\"f1-score\"] for k, v in report.items() if isinstance(v, dict)])\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": round(acc, 4),\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1 Score\": round(f1, 4)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a47e7b7-b750-4262-bf6f-28f5bea216b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "         ================================\n",
      "           Model Performance Comparison\n",
      "         ================================\n",
      "        Model  Accuracy  Precision  Recall  F1 Score\n",
      "      XGBoost     0.816     0.8188  0.8185    0.8178\n",
      "Random Forest     0.812     0.8121  0.8144    0.8121\n",
      "     LightGBM     0.811     0.8131  0.8142    0.8127\n",
      "Decision Tree     0.727     0.7324  0.7303    0.7304\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results).sort_values(by=\"Accuracy\", ascending=False)\n",
    "print(\"\\n         ================================\")\n",
    "print(\"           Model Performance Comparison\")\n",
    "print(\"         ================================\")\n",
    "print(results_df.to_string(index=False))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28cf91b4-e112-468a-a0ed-1ee4c76c12a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: encode target labels if needed\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = XGBClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=300,     # More trees for better performance\n",
    "    learning_rate=0.1,    # Controls speed/accuracy tradeoff\n",
    "    max_depth=6,          # Controls model complexity\n",
    "    subsample=0.8,        # Helps prevent overfitting\n",
    "    colsample_bytree=0.8, # Random feature sampling per tree\n",
    "    eval_metric=\"mlogloss\" # Recommended for classification\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Save both model and label encoder\n",
    "with open(\"xgboost_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "\n",
    "print(\"✅ XGBoost model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ed1fe-3e46-4186-ba32-49bf61bea38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
